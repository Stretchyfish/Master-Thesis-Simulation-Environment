# Based on
# https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit
# https://www.tensorflow.org/tensorboard/get_started

import time
import webbrowser
import tensorflow as tf
from tensorflow import keras
import numpy as np
#from tensorflow.keras.callbacks import TensorBoard
from keras.callbacks import TensorBoard
from tensorboard import program

logdir="logs/fit/my_model"
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)

class CustomModel(keras.Model):
    def train_step(self, data):
        # Unpack the data. Its structure depends on your model and
        # on what you pass to `fit()`.
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

# Construct and compile an instance of CustomModel
inputs = keras.Input(shape=(3,))
midLayer = keras.layers.Dense(10, activation='sigmoid')(inputs)
#midLayer2 = keras.layers.Dense(10, activation='sigmoid')(midLayer)
#midLayer3 = keras.layers.Dense(10, activation='sigmoid')(midLayer2)
outputs = keras.layers.Dense(4,  activation='sigmoid')(midLayer)
model = CustomModel(inputs, outputs)
model.compile(optimizer="adam", loss="mse", metrics=["mae"])

x = np.array([[-10 , 1, 0], [-9.5 , 1, 0], [-9 , 1, 0], [-8.5 , 1, 0], [-8 , 1, 0], [-7.5 , 1, 0], [-7 , 1, 0], [-6.5 , 1, 0], [-6 , 1, 0], [-5.5 , 1, 0], [-5 , 1, 0], [-4.5 , 1, 0], [-4 , 1, 0], [-3.5 , 1, 0], [-3 , 1, 0], [-2.5 , 1, 0], [-2 , 1, 0], [-1.5 , 1, 0], [-1 , 1, 0], [-0.5 , 1, 0], [0 , 1, 0], [0.5 , 1, 0], [1 , 1, 0], [1.5 , 1, 0], [2 , 1, 0], [2.5 , 1, 0], [3 , 1, 0], [3.5 , 1, 0], [4 , 1, 0], [4.5 , 1, 0], [5 , 1, 0], [5.5 , 1, 0], [6 , 1, 0], [6.5 , 1, 0], [7 , 1, 0], [7.5 , 1, 0], [8 , 1, 0], [8.5 , 1, 0], [9 , 1, 0], [9.5 , 1, 0], [10 , 1, 0], [-10 , 0, 1], [-9.5 , 0, 1], [-9 , 0, 1], [-8.5 , 0, 1], [-8 , 0, 1], [-7.5 , 0, 1], [-7 , 0, 1], [-6.5 , 0, 1], [-6 , 0, 1], [-5.5 , 0, 1], [-5 , 0, 1], [-4.5 , 0, 1], [-4 , 0, 1], [-3.5 , 0, 1], [-3 , 0, 1], [-2.5 , 0, 1], [-2 , 0, 1], [-1.5 , 0, 1], [-1 , 0, 1], [-0.5 , 0, 1], [0 , 0, 1], [0.5 , 0, 1], [1 , 0, 1], [1.5 , 0, 1], [2 , 0, 1], [2.5 , 0, 1], [3 , 0, 1], [3.5 , 0, 1], [4 , 0, 1], [4.5 , 0, 1], [5 , 0, 1], [5.5 , 0, 1], [6 , 0, 1], [6.5 , 0, 1], [7 , 0, 1], [7.5 , 0, 1], [8 , 0, 1], [8.5 , 0, 1], [9 , 0, 1], [9.5 , 0, 1], [10 , 0, 1]])


#print(x)

y = np.array([[0.995139718055725 , 0.0253825038671494 , 0.463771343231201 , 0.998518407344818 ], [0.993797540664673 , 0.0295038390904665 , 0.477695256471634 , 0.997857570648193 ], [0.992087721824646 , 0.0342708267271519 , 0.491653889417648 , 0.996902823448181 ], [0.989911317825317 , 0.0397764518857002 , 0.505625605583191 , 0.995524525642395 ], [0.98714405298233 , 0.0461243242025375 , 0.519588470458984 , 0.993536829948425 ], [0.983630299568176 , 0.0534288585186005 , 0.533520817756653 , 0.990674674510956 ], [0.97917640209198 , 0.0618152283132076 , 0.547401130199432 , 0.986562132835388 ], [0.973543226718903 , 0.0714186504483223 , 0.561208009719849 , 0.980671286582947 ], [0.966438472270966 , 0.0823829919099808 , 0.574920833110809 , 0.972270667552948 ], [0.95750904083252 , 0.0948586687445641 , 0.588519334793091 , 0.960366487503052 ], [0.946335792541504 , 0.10899917781353 , 0.601983964443207 , 0.943648040294647 ], [0.932431817054749 , 0.124956578016281 , 0.615296006202698 , 0.920461416244507 ], [0.915247976779938 ,
0.142875552177429 , 0.628437638282776 , 0.888858139514923 ], [0.894189894199371 , 0.162885800004005 , 0.641391932964325 , 0.846787869930267 ], [0.868650496006012 , 0.185093373060226 , 0.654143214225769 , 0.792510211467743 ], [0.838063061237335 , 0.209570690989494 , 0.666676700115204 , 0.725243031978607 ], [0.801976442337036 , 0.236346155405045 , 0.678978860378265 , 0.645911753177643 ], [0.760149598121643 , 0.265393942594528 , 0.69103741645813 , 0.557645916938782 ], [0.712653577327728 , 0.296625077724457 , 0.702841222286224 , 0.465582072734833 ], [0.659959495067596 , 0.32988104224205 , 0.714380502700806 , 0.375804871320724 ], [0.602985560894012 , 0.364931046962738 , 0.725646555423737 , 0.29382136464119 ], [0.5430748462677 , 0.401473999023438 , 0.736632108688354 , 0.223324045538902 ], [0.481891542673111 , 0.439145892858505 , 0.747330904006958 , 0.165770724415779 ], [0.421246469020844 , 0.477532356977463 , 0.757738053798676 , 0.120743729174137 ], [0.362887889146805 , 0.516186058521271 , 0.767849683761597 , 0.086676336824894 ], [0.308307290077209 , 0.554647028446198 , 0.777663111686707 , 0.0615481100976467 ], [0.25860384106636 , 0.592464506626129 , 0.787176728248596 , 0.04335892572999 ], [0.214429214596748 , 0.629217386245728 , 0.796389758586884 , 0.0303711872547865 ], [0.176009058952332 , 0.664532542228699 , 0.805302679538727 , 0.0211876351386309 ], [0.143217816948891 , 0.698097944259644 , 0.813916563987732 , 0.0147387683391571 ], [0.115678034722805 , 0.729671180248261 , 0.822233498096466 , 0.0102322176098824 ], [0.092860035598278 , 0.759081959724426 , 0.830256223678589 , 0.00709368009120226 ], [0.0741655007004738 , 0.786230385303497 , 0.837988257408142 , 0.00491304835304618 ], [0.0589897856116295 , 0.811080873012543 , 0.845433592796326 , 0.00340045941993594 ], [0.0467624925076962 , 0.833653748035431 , 0.852596998214722 , 0.00235245167277753 ], [0.036970067769289 , 0.854014992713928 , 0.859483599662781 , 0.00162690912839025 ], [0.0291655156761408 , 0.872265815734863 , 0.866098940372467 , 0.00112488551530987 ], [0.0229692477732897 , 0.888532757759094 , 0.872449100017548 , 0.000777653476689011 ], [0.0180648900568485 , 0.902958571910858 , 0.878540337085724 , 0.000537547864951193 ], [0.0141924871131778 , 0.91569459438324 , 0.884379208087921 , 0.000371549045667052 ], [0.0111407609656453 , 0.926894426345825 , 0.889972567558289 , 0.000256798492046073 ], [0.373514831066132 , 0.463379472494125 , 0.865650296211243 , 0.914895832538605 ], [0.37251889705658 , 0.456312388181686 , 0.860974133014679 , 0.903581976890564 ], [0.371524065732956 , 0.449262857437134 , 0.856162250041962 , 0.890943348407745 ], [0.370530307292938 , 0.442233681678772 , 0.851212620735168 , 0.876873731613159 ], [0.369537621736526 , 0.435227543115616 , 0.846123278141022 , 0.861271619796753 ], [0.368546068668365 , 0.428247183561325 , 0.840892374515533 , 0.844044089317322 ], [0.367555648088455 , 0.421295285224915 , 0.83551824092865 , 0.825111627578735 ], [0.366566300392151 , 0.414374440908432 , 0.829999268054962 , 0.804413378238678 ], [0.365578085184097 , 0.407487213611603 , 0.82433408498764 , 0.781912982463837 ], [0.364591002464294 , 0.400636166334152 , 0.8185213804245 , 0.757604122161865 ], [0.363605052232742 , 0.393823713064194 , 0.812560021877289 , 0.731516063213348 ], [0.362620264291763 , 0.387052327394485 , 0.806449174880981 , 0.703718304634094 ], [0.361636608839035 , 0.380324304103851 , 0.800188064575195 , 0.674323856830597 ], [0.36065411567688 , 0.373641967773438 , 0.793776214122772 , 0.643490493297577 ], [0.35967281460762 , 0.367007464170456 , 0.787213385105133 , 0.611420214176178 ], [0.358692675828934 , 0.360422998666763 , 0.780499398708344 , 0.578355252742767 ], [0.357713669538498 , 0.353890627622604 , 0.773634493350983 , 0.544572532176971 ], [0.356735914945602 , 0.347412317991257 , 0.766619145870209 , 0.510374486446381 ], [0.35575932264328 , 0.340990036725998 , 0.759453952312469 , 0.476079136133194 ], [0.354783922433853 , 0.334625571966171 , 0.752139866352081 , 0.44200786948204 ], [0.353809714317322 , 0.328320771455765 , 0.744678139686584 , 0.408473908901215 ], [0.352836757898331 , 0.322077214717865 , 0.737070322036743 , 0.375770747661591 ], [0.351864993572235 , 0.315896570682526 , 0.729318261146545 , 0.34416252374649 ], [0.350894451141357 , 0.309780299663544
, 0.721423983573914 , 0.313876181840897 ], [0.34992516040802 , 0.303729891777039 , 0.713389933109283 , 0.285096406936646 ], [0.348957091569901 , 0.297746688127518 , 0.705218911170959 , 0.257963418960571 ], [0.347990304231644 , 0.291831940412521 , 0.69691389799118 , 0.232572600245476 ], [0.347024708986282 , 0.285986840724945 , 0.688478231430054 , 0.20897713303566 ], [0.346060395240784 , 0.280212491750717 , 0.679915606975555 , 0.187191605567932 ], [0.345097362995148 , 0.274509906768799 , 0.67123007774353 , 0.167197152972221 ], [0.344135582447052 , 0.268880009651184 , 0.662425816059113 , 0.148947015404701 ], [0.343175083398819 , 0.263323694467545 , 0.65350753068924 , 0.13237227499485 ], [0.342215865850449 , 0.25784170627594 , 0.644480049610138 , 0.11738757789135 ], [0.341257929801941 , 0.252434700727463 , 0.635348677635193 , 0.103896051645279 ], [0.340301305055618 , 0.247103348374367 , 0.626118719577789 , 0.0917938500642776 ], [0.339345961809158 , 0.241848155856133 , 0.616796135902405 , 0.0809739604592323 ], [0.338391929864883 , 0.236669614911079 , 0.607386827468872 , 0.0713292807340622 ], [0.337439209222794 , 0.231568083167076 , 0.597897112369537 , 0.0627549141645432 ], [0.336487829685211 , 0.226543873548508 , 0.588333487510681 , 0.0551500394940376 ], [0.335537731647491 , 0.221597254276276 , 0.578702628612518 , 0.0484191626310349 ], [0.334588974714279 , 0.216728374361992 , 0.569011569023132 , 0.0424728207290173]])
#print(y)

tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)

model.fit(x, y, epochs=2000, callbacks=[tensorboard_callback])

scores = model.evaluate(x, y)

model.save("./logs/train")

print(model.predict([[10, 0, 1]]))
print(model.predict([[10, 1, 0]]))

print("test")

check = model.predict([[10, 1, 0]])
print(check[0, 1])

funk1 = "["
funk2 = "["
funk3 = "["
funk4 = "["

funk5 = "["
funk6 = "["
funk7 = "["
funk8 = "["

val1 = []
val2 = []
val3 = []
val4 = []
val5 = []
val6 = []
val7 = []
val8 = []

split = 20 # 800
x = np.linspace(-10, 10, split)

print("Got here2")
for i in range(1, len(x) + 1):
    print(i)
    temp = -10 + (20/split) * i
    values1 = model.predict([[temp, 1, 0]])
    values2 = model.predict([[temp, 0, 1]])

    funk1 += str(values1[0, 0]) + ", "
    funk2 += str(values1[0, 1]) + ", "
    funk3 += str(values1[0, 2]) + ", "
    funk4 += str(values1[0, 3]) + ", "

    val1.append(values1[0, 0])
    val2.append(values1[0, 1])
    val3.append(values1[0, 2])
    val4.append(values1[0, 3])

    funk5 += str(values2[0, 0]) + ", "
    funk6 += str(values2[0, 1]) + ", "
    funk7 += str(values2[0, 2]) + ", "
    funk8 += str(values2[0, 3]) + ", "

    val5.append(values2[0, 0])
    val6.append(values2[0, 1])
    val7.append(values2[0, 2])
    val8.append(values2[0, 3])

print(funk1)
print(funk2)
print(funk3)
print(funk4)
print(funk5)
print(funk6)
print(funk7)
print(funk8)

import matplotlib.pyplot as plt

print("Got here")

print(len(x))
print(len(val1))
print("Length above")

fig, axs = plt.subplots(2, 4)
axs[0, 0].plot(x, val1)
axs[0, 0].set_title('Axis [0, 0]')
axs[0, 0].set_xlim([-10, 10])
axs[0, 0].set_ylim([0, 1])

axs[0, 1].plot(x, val2, 'tab:orange')
axs[0, 1].set_title('Axis [0, 1]')
axs[0, 1].set_xlim([-10, 10])
axs[0, 1].set_ylim([0, 1])

axs[0, 2].plot(x, val3, 'tab:green')
axs[0, 2].set_title('Axis [1, 0]')
axs[0, 2].set_xlim([-10, 10])
axs[0, 2].set_ylim([0, 1])

axs[0, 3].plot(x, val4, 'tab:red')
axs[0, 3].set_title('Axis [1, 1]')
axs[0, 3].set_xlim([-10, 10])
axs[0, 3].set_ylim([0, 1])

axs[1, 0].plot(x, val5)
axs[1, 0].set_title('Axis [0, 0]')
axs[1, 0].set_xlim([-10, 10])
axs[1, 0].set_ylim([0, 1])

axs[1, 1].plot(x, val6, 'tab:orange')
axs[1, 1].set_title('Axis [0, 1]')
axs[1, 1].set_xlim([-10, 10])
axs[1, 1].set_ylim([0, 1])

axs[1, 2].plot(x, val7, 'tab:green')
axs[1, 2].set_title('Axis [1, 0]')
axs[1, 2].set_xlim([-10, 10])
axs[1, 2].set_ylim([0, 1])

axs[1, 3].plot(x, val8, 'tab:red')
axs[1, 3].set_title('Axis [1, 1]')
axs[1, 3].set_xlim([-10, 10])
axs[1, 3].set_ylim([0, 1])


"""
from tensorboard import program

tracking_address = "logs/fit/my_model" # the path of your log file.

tb = program.TensorBoard()
tb.configure(argv=[None, '--logdir', tracking_address])
url = tb.launch()
print(f"Tensorflow listening on {url}")

while True:
    pass
"""

log_directory = "logs\\train"

#tb = program.TensorBoard()
#tb.configure(argv=[None, '--logdir', log_directory])
#url = tb.launch()
#webbrowser.open(url)
#time.sleep(120)
#tb.kill()

for layer in model.layers:
    print(layer.get_weights())